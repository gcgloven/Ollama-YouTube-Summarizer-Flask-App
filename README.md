a flask app with Ollama based YouTube Summarizer - a tool build while I learn agentic AI
## YouTube Summarizer (Local LLM / Whisper / Ollama)

A Flask application that summarizes YouTube videos using:
- **Official YouTube transcripts** (if available) or **local Whisper** transcription as a fallback.
- **Locally running Ollama** server for LLM-based summarization.
- **Optional** custom prompts, with SSE-based partial streaming of summary and title tokens.
- Full CRUD for:
  - Summaries
  - Prompts (except the protected default)
- A user-friendly interface with pagination and dedicated record pages for each summary.

---

## Table of Contents
- [Features](#features)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [FAQ](#faq)
- [License](#license)

---

## Features

1. **Local LLM**: Integrates with an [Ollama](https://github.com/jmorganca/ollama) server to run models like Llama 2 or GPT4All locally.  
2. **Whisper-based Transcription**: Automatically downloads audio with [yt-dlp](https://github.com/yt-dlp/yt-dlp) and uses [OpenAI Whisper](https://github.com/openai/whisper) if YouTube transcripts are missing.  
3. **Prompt Management**: Create, edit, and delete custom prompts to shape your summaries.  
4. **Token-by-Token Streaming**: The UI displays partial tokens as they’re generated by the LLM.  
5. **Pagination**: Choose how many summaries to display per page (5/10/25/50).  
6. **Dedicated Summary Pages**: Click a summary’s title to see a standalone view.

APP VIEW
![APP VIEW](https://i.imgur.com/hty0wiI.png)

---

## Project Structure
```graphql
my_summarizer/
├── run.py                      # Entry point to run the Flask app
├── my_summarizer/
│   ├── __init__.py            # Creates the Flask app, registers blueprints
│   ├── routes/
│   │   ├── __init__.py
│   │   ├── main.py            # Home page, SSE logic, streaming
│   │   ├── prompt.py          # Prompt CRUD routes
│   │   └── summary.py         # Summary CRUD routes
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── ollama_client.py   # Ollama client config (host, list models)
│   │   ├── storage.py         # JSON loading/saving for prompts & summaries
│   │   └── transcript.py      # YouTube transcript & Whisper logic
│   ├── templates/
│   │   ├── base.html
│   │   ├── index.html         # Main SSE page
│   │   ├── manage_prompts.html
│   │   ├── edit_prompt.html
│   │   ├── edit_summary.html
│   │   └── view_summary.html
│   └── static/
│       ├── css/
│       │   └── style.css
│       └── js/
│           └── main.js        # JavaScript for SSE streaming & UI behaviors
└── cache/
    ├── summaries.json         # Saved summary records
    ├── prompts.json           # Saved prompt records
    └── [transcripts, audio]   # Various cached files

```
Key directories:
- my_summarizer/routes: Each file is a Blueprint handling different parts of the app.
- my_summarizer/utils: Helper modules for storing data, calling Ollama, or managing transcripts.
- my_summarizer/templates: All HTML templates, separated for maintainability.
- my_summarizer/static: Custom JS, CSS, or other static files.
- cache/: Stores JSON data (summaries.json, prompts.json) and downloaded transcripts/audio.

## Requirements
1. Python 3.7+ (Tested with 3.9+)
2. pip or conda for package installation
3. **Dependencies**:
   - [Flask](https://palletsprojects.com/p/flask/)
   - [yt-dlp](https://github.com/yt-dlp/yt-dlp)
   - [Whisper](https://github.com/openai/whisper)
   - [youtube-transcript-api](https://github.com/jdepoix/youtube-transcript-api)
   - [ollama.py](https://github.com/jianjie91/ollama.py) (for local LLM integration)
4. A running **[Ollama](https://github.com/jmorganca/ollama)** server with your local model(s) installed.
5. **[ffmpeg](https://www.ffmpeg.org/)** for video download, when transcript is not available (Optional)

## Installation

1. **Clone** the repository:
   ```bash
   git clone https://github.com/yourusername/my_summarizer.git
   cd my_summarizer
2. **Create** a virtual environment (recommended):
```
python -m venv venv
source venv/bin/activate   # Linux/Mac
# or .\venv\Scripts\activate  # Windows
```
3. Install dependencies:
```
pip install flask yt_dlp whisper youtube_transcript_api ollama openai-whisper ffmepg markdown
``` 
4. Check that the cache/ directory exists (it should be created automatically if missing).

## Usage
1. Start Ollama server
Install some example small models based on you PC specs:
``` bash
ollama run qwen2.5:7b
ollama run tulu3 #8B
ollama run deepseek-r1 #7B
```
Make sure it’s listening on http://127.0.0.1:11434 (or adjust in ollama_client.py if different).

2. Run flask app
``` bash
python run.py
```

By default, it starts on port **5000**.

3. Visit http://127.0.0.1:5000 in your browser.

4. Add a new summary:
    - Enter a YouTube URL (e.g., https://youtu.be/...).
    - Select a local LLM model (e.g., llama2).
    - Choose a prompt (e.g., “Default Summarizing Prompt”).
    - Click “Stream Summarize”.
    - You’ll see partial tokens appear for the summary and title.
    - When done, the page reloads, and the new summary is listed in “Past Summaries.”

5. Manage Prompts (optional):
    - Go to “Manage Prompts” (http://127.0.0.1:5000/prompts).
    - Add your own custom summarization style or constraints.

6. Pagination:
    - On the home page, pick 5/10/25/50 records per page to control how many summaries appear at once.
7. View a record’s dedicated page:
    - Click the Title in the table to see a single summary page at /summaries/<id>.

## Configuration
Port and debug mode: Adjust in run.py
``` python
app.run(debug=True, port=5000)
```

Ollama host: Edit my_summarizer/utils/ollama_client.py:
```python
OLLAMA_HOST = "http://127.0.0.1:11434"
```

Cache directory: By default, stored as a sibling to run.py named cache/.
If you move it, update the path references in storage.py and transcript.py.


## FAQ
1. Why do I get “Could not get transcripts”?

    YouTube transcripts might be disabled or unavailable. The app automatically falls back to Whisper, but if Whisper also fails, check that ffmpeg and whisper are installed properly.

2. How do I install other LLMs for Ollama?

    Refer to Ollama docs for commands like ollama pull llama2-7b. Then restart the server, and your model should appear in the model list.

3. Why do partial tokens show weird spacing or punctuation?
    
    Streaming tokenization can produce odd partial strings. The final text is combined in the back end. If you want a more polished version, consider trimming or reformatting the text once the summary is done.

4. Can I do server-side Markdown conversion for the summary?
    
    Yes, install markdown (pip install markdown), parse record["summary"] in the route, and pass it as summary_html to the template using {{ summary_html|safe }}.

5. Why the title sometime a bit qurky?
   It is LLM generated, up to you to fine tune the title_prompt.

## License
This project is distributed under the MIT License. Feel free to modify and share. If you build something neat with it, let us know!